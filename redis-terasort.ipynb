{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pywren\n",
    "import numpy as np\n",
    "from boto.s3.connection import S3Connection\n",
    "from boto.s3.key import Key\n",
    "import pywren.storage as storage\n",
    "import boto3\n",
    "import pickle \n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# configuration \n",
    "num_workers = 3\n",
    "bucket_name = 'terasort-yawen'\n",
    "\n",
    "# the file to be sorted should be partitioned into \"num_worker\" number of files \n",
    "# as inputs to the map stage; \n",
    "# specify directory that contains files to be sorted: input1, input2, etc. \n",
    "path_local = 'input_files/' \n",
    "path = \"input_3_3M/\"\n",
    "file_name = 'input'\n",
    "concat_file_name = path_local + path + file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# upload n input files to S3 (inputs to the mapper stage)\n",
    "for i in range(num_workers):\n",
    "    result = s3_client.put_object(\n",
    "        Bucket = bucket_name,\n",
    "        Body = open(concat_file_name + str(i), 'rb'),\n",
    "        Key = path + file_name + str(i)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## sample[i âˆ’ 1] <= key < sample[i] is sent to reduce i\n",
    "def get_sample_keys(file_path, num_workers):\n",
    "    f = open(file_path, \"r\")\n",
    "    lines = f.readlines()\n",
    "\n",
    "    key_list = []\n",
    "\n",
    "    for line in lines: \n",
    "        data = line.split(\"  \")\n",
    "        key = data[0]\n",
    "        key_list.append(key)\n",
    "\n",
    "    key_list.sort()\n",
    "    length = len(key_list)\n",
    "    print \"num records: \" + str(length)\n",
    "    n = num_workers\n",
    "    key_range = length/n\n",
    "    index = 0\n",
    "    sample_key_list = []\n",
    "    for i in range(1, n+1): \n",
    "        if (i==n):\n",
    "            index = length -1\n",
    "            sample_key_list.append(key_list[length-1])\n",
    "        else:\n",
    "            index += key_range\n",
    "            sample_key_list.append(key_list[index])\n",
    "        print index\n",
    "    \n",
    "    return sample_key_list\n",
    "\n",
    "#get_sample_keys('input_files/input0', num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "# partition stage: partition input data into n groups \n",
    "def mapper(data):\n",
    "    import os\n",
    "    from redis import Redis\n",
    "\n",
    "    \n",
    "    id = data[0]\n",
    "    n = num_workers = data[1]\n",
    "    bucket_name = data[2]\n",
    "    sample_keys = data[3]\n",
    "    path = data[4]\n",
    "\n",
    "    t0=time.time()\n",
    "    #[s3] read from input file: input<id> \n",
    "    s3 = boto3.resource('s3')\n",
    "    key = path + 'input' + str(id)\n",
    "    file_local = '/tmp/input_tmp'\n",
    "    s3.Bucket(bucket_name).download_file(key, file_local)\n",
    "    t1=time.time()    \n",
    "        \n",
    "    #partition \n",
    "    with open(file_local, \"r\") as f: \n",
    "        lines = f.readlines() #each line contains a 100B record\n",
    "    os.remove(file_local)\n",
    "    p_list = [[] for x in xrange(n)]  #list of n partitions\n",
    "    for line in lines:\n",
    "        data = line.split(\"  \")\n",
    "        '''\n",
    "        if len(data) != 3:\n",
    "            data[0] = data[0]+data[1]\n",
    "            data[1] = data[2]\n",
    "            data[2] = data[3]\n",
    "        '''\n",
    "        index = 0\n",
    "        while data[0] > sample_keys[index]:\n",
    "            index += 1\n",
    "        p_list[index].append(line)\n",
    "    t2=time.time()\n",
    "    \n",
    "    \n",
    "    #write to output files in tmp: shuffle<id 0> shuffle<id 1> shuffle<id num_workers-1>\n",
    "    f_list = [] #output file list\n",
    "    redis_client = redis.Redis(host=\"rediscluster.a9ith3.clustercfg.usw2.cache.amazonaws.com\", port=6379)\n",
    "    for i in range(n):\n",
    "        key = 'tmp/shuffle' + str(id) + str(i)\n",
    "        result = redis_client.set(key, pickle.dumps(p_list[i]))\n",
    "    t3 = time.time()\n",
    "    \n",
    "    #return time spent (in sec) writing intermediate files \n",
    "    return [t1-t0, t2-t1, t3-t2] #read input, compute, write shuffle \n",
    "\n",
    "#mapper([2, num_workers, bucket_name, sample_keys])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "# sort stage: merge n sets of data & sort \n",
    "def reducer(data):\n",
    "    import os\n",
    "    from redis import Redis\n",
    "    \n",
    "    id = data[0]\n",
    "    n = num_workers = data[1]\n",
    "    bucket_name = data[2]\n",
    "    \n",
    "    #read from input file in tmp: shuffle<0 id> shuffle<1 id> ... shuffle<id num_workers-1>\n",
    "    t0 = time.time()\n",
    "    redis_client = redis.Redis(host=\"rediscluster.a9ith3.clustercfg.usw2.cache.amazonaws.com\", port=6379)\n",
    "    lines_list = []\n",
    "    for i in range(n):\n",
    "        key = 'tmp/shuffle'+ str(i) + str(id)\n",
    "        body = redis_client.get(key)\n",
    "        if body == None:\n",
    "            return -1\n",
    "        lines = pickle.loads(body)\n",
    "        lines_list.append(lines)\n",
    "    t1 = time.time()\n",
    "    \n",
    "    #merge & sort \n",
    "    merged_lines = sum(lines_list, [])\n",
    "    tuples_list = []\n",
    "    for line in merged_lines:\n",
    "        data = line.split('  ')\n",
    "        tuples_list.append((data[0], data[1]+'  '+data[2]))\n",
    "    \n",
    "    sorted_tuples_list = sorted(tuples_list, key=lambda x: x[0])\n",
    "    t2=time.time()\n",
    "    \n",
    "    #[s3] write to output file: output<id>  \n",
    "    with open('/tmp/sorted_output', 'w+') as f:\n",
    "        for t in sorted_tuples_list: \n",
    "            f.write(t[0]+'  '+t[1])\n",
    "    \n",
    "    s3_client = boto3.client('s3')\n",
    "    result = s3_client.put_object(\n",
    "        Bucket = bucket_name,\n",
    "        Body = open('/tmp/sorted_output', 'rb'),\n",
    "        Key = 'output/sorted_output' + str(id)\n",
    "    )\n",
    "    t3=time.time()\n",
    "    \n",
    "    #return time (in sec) spent reading intermediate files\n",
    "    return [t1-t0, t2-t1, t3-t2] #read shuffle, compute, write output \n",
    "\n",
    "#reducer([0, num_workers, bucket_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def final_reducer(data):\n",
    "    import os\n",
    "    \n",
    "    t0=time.time()\n",
    "    n = data[0]\n",
    "    bucket_name = data[1]\n",
    "    \n",
    "    s3 = boto3.resource('s3')\n",
    "\n",
    "    for i in range(n):\n",
    "        key = 'output/sorted_output'+ str(i)\n",
    "        file_local = 'output/sorted_output'+ str(i)\n",
    "        s3.Bucket(bucket_name).download_file(key, file_local)        \n",
    "    \n",
    "    # concatenate all files \n",
    "    \n",
    "    \n",
    "    t1=time.time()\n",
    "    return (t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "wrenexec = pywren.default_executor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num records: 30000\n",
      "10000\n",
      "20000\n",
      "29999\n"
     ]
    }
   ],
   "source": [
    "map_data_list = []\n",
    "reduce_data_list = []\n",
    "\n",
    "sample_keys = get_sample_keys(concat_file_name, num_workers)\n",
    "\n",
    "for i in range(num_workers):\n",
    "    map_data_list.append([i, num_workers, bucket_name, sample_keys, path])\n",
    "    reduce_data_list.append([i, num_workers, bucket_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "futures = wrenexec.map(mapper, map_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-fd124ad56cee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywren\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/Yawen/anaconda2/lib/python2.7/site-packages/pywren/wren.pyc\u001b[0m in \u001b[0;36mget_all_results\u001b[0;34m(fs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mWill\u001b[0m \u001b[0mthrow\u001b[0m \u001b[0man\u001b[0m \u001b[0mexception\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0mthrew\u001b[0m \u001b[0man\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \"\"\"\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_when\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mALL_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Yawen/anaconda2/lib/python2.7/site-packages/pywren/wait.pyc\u001b[0m in \u001b[0;36mwait\u001b[0;34m(fs, return_when, THREADPOOL_SIZE, WAIT_DUR_SEC)\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfs_dones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs_notdones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWAIT_DUR_SEC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mreturn_when\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mANY_COMPLETED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results_map = pywren.get_all_results(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "futures = wrenexec.map(reducer, reduce_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_reduce = pywren.get_all_results(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map:\n",
      "read input: 0.21368598938\n",
      "compute: 0.0870207150777\n",
      "write inter: 3.1236846447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.21266603469848633, 0.09230589866638184, 3.2602360248565674],\n",
       " [0.23511099815368652, 0.07327914237976074, 2.75010085105896],\n",
       " [0.19328093528747559, 0.09547710418701172, 3.3607170581817627]]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_io = []\n",
    "t_comp = []\n",
    "t_inter = []\n",
    "for r in results_map:\n",
    "    t_io.append(r[0])\n",
    "    t_comp.append(r[1])\n",
    "    t_inter.append(r[2])\n",
    "print \"map:\"\n",
    "print \"read input: \" + str(sum(t_io) / len(t_io))\n",
    "print \"compute: \" + str(sum(t_comp) / len(t_comp))\n",
    "print \"write inter: \" + str(sum(t_inter) / len(t_inter))\n",
    "\n",
    "# returns time spent (in sec) writing intermediate data in each mapper \n",
    "results_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduce:\n",
      "read inter: 3.55727163951\n",
      "compute: 0.679601669312\n",
      "write output: 7.11767133077\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[4.683604001998901, 1.3008201122283936, 7.160967826843262],\n",
       " [1.5085079669952393, 0.29518890380859375, 7.957313060760498],\n",
       " [4.479702949523926, 0.442795991897583, 6.2347331047058105]]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# returns time spent (in sec) reading intermediate data in each reducer \n",
    "\n",
    "t_io = []\n",
    "t_comp = []\n",
    "t_inter = []\n",
    "for r in results_reduce:\n",
    "    t_io.append(r[2])\n",
    "    t_comp.append(r[1])\n",
    "    t_inter.append(r[0])\n",
    "print \"reduce:\"\n",
    "print \"read inter: \" + str(sum(t_inter) / len(t_inter))\n",
    "print \"compute: \" + str(sum(t_comp) / len(t_comp))\n",
    "print \"write output: \" + str(sum(t_io) / len(t_io))\n",
    "results_reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# final stage: concatenate outputs from the reduce/sort stage to form a single sorted output file\n",
    "#final_reducer([num_workers,bucket_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
